# -*- coding: utf-8 -*-
"""AI-Driven ESG risk assessment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xt-30AfjS1nbusUQUSeeEjvnh-K_Tvji
"""

import pdfplumber
import pandas as pd
from textblob import TextBlob

pip install pdfplumber

esg_terms = {
    "Environmental": ["emission", "pollution", "waste", "carbon", "climate", "deforestation"],
    "Social": ["labor", "human rights", "discrimination", "diversity", "safety", "community"],
    "Governance": ["fine", "lawsuit", "corruption", "fraud", "regulation", "penalty"]
}

def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:  # some pages may be empty
                text += page_text + " "
    return text.lower()

def analyze_report(company, year, pdf_path):
    text = extract_text_from_pdf(pdf_path)

    records = []
    total_score = 0
    for category, keywords in esg_terms.items():
        for word in keywords:
            count = text.count(word)
            if count > 0:
                # Sentiment score for the whole report (basic check)
                sentiment = TextBlob(text).sentiment.polarity
                total_score += count
                records.append([company, year, word, category, count, sentiment])

    return records, total_score

all_records = []
company_scores = []

reports = [
    ("HoneyWell", 2023, "/content/hon-esg-report.pdf"),
    ("Infosys", 2023, "/content/infosys-esg-report-2022-23.pdf"),
    ("Adani", 2023, "/content/latest report.pdf"),
    ("Wipro", 2023, "/content/wipro-sustainability-report-fy-2023-2024.pdf")
]

for company, year, path in reports:
    records, score = analyze_report(company, year, path)
    all_records.extend(records)
    company_scores.append([company, year, score])

df = pd.DataFrame(all_records, columns=["Company", "Year", "Risk_Term", "Category", "Frequency", "Sentiment_Score"])
df_scores = pd.DataFrame(company_scores, columns=["Company", "Year", "Overall_ESG_Risk_Score"])
final_df = pd.merge(df, df_scores, on=["Company", "Year"], how="left")
final_df.to_excel("esg_risk_data.xlsx", index=False)

print("✅ ESG dataset for 4 companies saved as 'esg_risk_data.xlsx'")

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
warnings.filterwarnings("ignore")

import spacy
nlp = spacy.load("en_core_web_sm")

from transformers import pipeline
try:
    sentiment_pipe = pipeline("sentiment-analysis")
except Exception as e:
    sentiment_pipe = None
    print("Hugging Face sentiment pipeline not available. Install transformers and internet access to enable it.")

try:
    import pytesseract
    from PIL import Image
    OCR_AVAILABLE = True
except Exception:
    OCR_AVAILABLE = False

ESG_LEXICON = {
    "Environmental": [
        "emission", "emissions", "co2", "greenhouse", "ghg", "carbon",
        "pollution", "waste", "oil spill", "spill", "deforestation",
        "renewable", "efficiency", "energy consumption", "effluent"
    ],
    "Social": [
        "labor", "child labor", "human rights", "discrimination",
        "diversity", "harassment", "safety", "injury", "strike", "community",
        "privacy", "data breach"
    ],
    "Governance": [
        "corruption", "fraud", "bribery", "lawsuit", "fine", "penalty",
        "investigation", "antitrust", "regulator", "board", "executive pay"
    ]
}

CATEGORY_WEIGHTS = {"Environmental": 1.5, "Social": 2.0, "Governance": 2.5}

MIN_TOKEN_LEN = 2

def extract_text_pdf_plumber(path):
    """Extract text using pdfplumber (works for most digital PDFs)."""
    text_parts = []
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text_parts.append(page_text)
    return "\n".join(text_parts).strip()

def extract_text_with_ocr(path):
    """Fallback: extract images and run OCR (if installed)."""
    if not OCR_AVAILABLE:
        raise RuntimeError("pytesseract not available for OCR.")
    text = ""
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            # some pages may have extract_text; prefer that first
            page_text = page.extract_text()
            if page_text and len(page_text.strip()) > 20:
                text += page_text + "\n"
            else:
                # render page to image and OCR it
                image = page.to_image(resolution=300).original
                ocr_text = pytesseract.image_to_string(image)
                text += ocr_text + "\n"
    return text
def extract_text(path, use_ocr_if_empty=True):
    """Extract text; fallback to OCR if initial extraction is empty and OCR is available."""
    text = extract_text_pdf_plumber(path)
    if (not text or len(text.strip()) < 20) and use_ocr_if_empty and OCR_AVAILABLE:
        text = extract_text_with_ocr(path)
    return text

def clean_text(raw):
    """Basic cleaning: normalize whitespace and remove weird characters."""
    if raw is None:
        return ""
    txt = re.sub(r"\u00A0", " ", raw)  # non-breaking space
    txt = re.sub(r"\s+", " ", txt)
    txt = txt.strip()
    return txt
def preprocess_tokens(text):
    """Return list of lemmatized tokens (lowercase) excluding stopwords/punct/numbers."""
    doc = nlp(text)
    tokens = []
    for tok in doc:
        if tok.is_stop or tok.is_punct or not tok.text.strip():
            continue
        if tok.is_alpha and len(tok.text) >= MIN_TOKEN_LEN:
            tokens.append(tok.lemma_.lower())
    return tokens
def sentences_from_text(text):
    """Split into spaCy sentences (raw strings)."""
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]

def detect_lexicon_flags(text):
    """
    Count occurrences of lexicon terms per category.
    Also capture example sentences for context.
    Returns:
      - counts: dict(category -> count)
      - details: list of (category, term, frequency, example_sentences)
    """
    normalized_text = text.lower()
    counts = defaultdict(int)
    details = []
    for cat, term_list in ESG_LEXICON.items():
        for term in term_list:
            term_lower = term.lower()
            if term_lower in normalized_text:
                freq = normalized_text.count(term_lower)
                counts[cat] += freq
                # get example sentences containing the term
                example_sents = [s for s in sentences_from_text(text) if term_lower in s.lower()]
                details.append({
                    "category": cat,
                    "term": term,
                    "frequency": freq,
                    "examples": example_sents[:3]  # up to 3 examples
                })
    return dict(counts), details
def score_from_counts(counts):
    """Compute weighted raw score from category counts."""
    total = 0.0
    for cat, cnt in counts.items():
        weight = CATEGORY_WEIGHTS.get(cat, 1.0)
        total += cnt * weight
    return float(total)
def normalize_score(raw_score, total_words, scale=100.0):
    """Normalize to remove bias from long reports. Returns 0..scale value."""
    if total_words <= 0:
        return raw_score
    # a simple normalization formula: (raw / total_words) * scale_factor
    norm = (raw_score / total_words) * (scale * 100)  # adjust multiplier for readable scale
    return float(round(norm, 2))
def sentiment_for_examples(example_sentences):
    """Return average polarity for a list of sentences using HF pipeline (if available)."""
    if not sentiment_pipe:
        # fallback using spaCy sentiment not available; return None
        return None
    scores = []
    for s in example_sentences:
        try:
            r = sentiment_pipe(s[:512])  # truncate long strings
            # transformer returns label and score; convert to numeric polarity estimate
            # map NEGATIVE -> -score, POSITIVE -> +score
            if isinstance(r, list) and r:
                label = r[0].get("label", "")
                sc = r[0].get("score", 0.0)
                if "NEG" in label.upper():
                    scores.append(-sc)
                else:
                    scores.append(sc)
        except Exception:
            continue
    if not scores:
        return None
    return float(np.mean(scores))

def analyze_reports(reports):
    """
    reports: list of tuples -> (company_name, year, filepath)
    returns: DataFrame with detailed rows per flagged term and summary_df
    """
    all_details = []
    summary_rows = []

    for company, year, path in reports:
        if not os.path.exists(path):
            print(f"Warning: file not found: {path}. Skipping.")
            continue

        print(f"Processing: {company} | {path}")
        raw = extract_text(path)
        raw = clean_text(raw)
        total_words = len(raw.split())

        counts, details = detect_lexicon_flags(raw)
        raw_score = score_from_counts(counts)
        normalized = normalize_score(raw_score, total_words, scale=100.0)

        # collect summary
        summary_rows.append({
            "Company": company,
            "Year": year,
            "Raw_Score": raw_score,
            "Normalized_Score": normalized,
            "Total_Words": total_words,
            "Environmental_Count": counts.get("Environmental", 0),
            "Social_Count": counts.get("Social", 0),
            "Governance_Count": counts.get("Governance", 0)
        })

        # for detailed rows per flagged term
        for det in details:
            avg_sent = sentiment_for_examples(det["examples"])
            all_details.append({
                "Company": company,
                "Year": year,
                "Category": det["category"],
                "Term": det["term"],
                "Frequency": det["frequency"],
                "Example_Sentences": " ||| ".join(det["examples"]) if det["examples"] else "",
                "Sentiment_Avg": avg_sent
            })

    df_details = pd.DataFrame(all_details)
    df_summary = pd.DataFrame(summary_rows)
    return df_details, df_summary

# ----------------------------
# 7. Visualization helpers
# ----------------------------
def plot_scores_bar(df_summary, score_col="Normalized_Score"):
    plt.figure(figsize=(8,5))
    sns.barplot(data=df_summary.sort_values(score_col, ascending=False),
                x="Company", y=score_col, palette="viridis")
    plt.title("Normalized ESG Risk Score by Company")
    plt.ylabel("Normalized Score")
    plt.xlabel("")
    plt.show()

def plot_category_stack(df_details):
    if df_details.empty:
        print("No details to plot.")
        return
    cat_summary = df_details.groupby(["Company","Category"])["Frequency"].sum().reset_index()
    plt.figure(figsize=(9,5))
    sns.barplot(data=cat_summary, x="Company", y="Frequency", hue="Category")
    plt.title("ESG Category Frequency by Company")
    plt.ylabel("Frequency")
    plt.show()

def plot_term_heatmap(df_details, top_n_terms=10):
    if df_details.empty:
        print("No details to plot.")
        return
    term_pivot = df_details.pivot_table(index="Company", columns="Term", values="Frequency", aggfunc="sum", fill_value=0)
    # reduce to top_n_terms overall
    col_sums = term_pivot.sum(axis=0).sort_values(ascending=False)
    top_cols = col_sums.index[:top_n_terms]
    small = term_pivot[top_cols]
    plt.figure(figsize=(12,4))
    sns.heatmap(small, annot=True, fmt="d", cmap="Reds")
    plt.title("Top Risk Term Frequency by Company")
    plt.show()

# ----------------------------
# 8. Example usage
# ----------------------------
if __name__ == "__main__":
    # Prepare list of 4 reports: (Company, Year, Path)
    reports = [
        ("HoneyWell", 2023, "/content/hon-esg-report.pdf"),
        ("Infosys", 2023, "/content/infosys-esg-report-2022-23.pdf"),
        ("Adani", 2023, "/content/latest report.pdf"),
        ("Wipro", 2023, "/content/wipro-sustainability-report-fy-2023-2024.pdf")
    ]

    df_details, df_summary = analyze_reports(reports)

    # Save outputs for Tableau
    os.makedirs("output", exist_ok=True)
    df_details.to_csv("output/esg_details.csv", index=False)
    df_summary.to_csv("output/esg_summary.csv", index=False)
    # Also save combined sheet in Excel
    with pd.ExcelWriter("output/esg_results.xlsx") as writer:
        df_summary.to_excel(writer, sheet_name="summary", index=False)
        df_details.to_excel(writer, sheet_name="details", index=False)

    print("Saved outputs to output/ folder:")
    print(" - esg_summary.csv  (company level)")
    print(" - esg_details.csv  (term-level)")
    print(" - esg_results.xlsx (Excel workbook)")

    # Quick plots for sanity-check
    if not df_summary.empty:
        plot_scores_bar(df_summary, score_col="Normalized_Score")
    if not df_details.empty:
        plot_category_stack(df_details)
        plot_term_heatmap(df_details, top_n_terms=8)

df_details, df_summary = analyze_reports(reports)

# Print summary of scores
print("\n=== ESG Risk Summary by Company ===\n")
print(df_summary[[
    "Company", "Year", "Raw_Score", "Normalized_Score",
    "Environmental_Count", "Social_Count", "Governance_Count"
]].to_string(index=False))

# Print top 5 flagged terms per company
print("\n=== Top Flagged ESG Terms (sample) ===\n")
if not df_details.empty:
    for company in df_details["Company"].unique():
        print(f"\n--- {company} ---")
        subset = df_details[df_details["Company"] == company]
        top_terms = subset.groupby("Term")["Frequency"].sum().sort_values(ascending=False).head(5)
        for term, freq in top_terms.items():
            print(f"{term}: {freq}")
else:
    print("No flagged terms found.")

# (Optional) Print some example sentences
print("\n=== Example Sentences for Flagged Risks ===\n")
if not df_details.empty:
    for i, row in df_details.head(5).iterrows():
        print(f"[{row['Company']} - {row['Category']} - {row['Term']}]")
        print(f"Examples: {row['Example_Sentences']}\n")

import re
import pandas as pd

# ✅ ESG Risk Lexicon
esg_terms = {
    "Environmental": ["emissions", "pollution", "deforestation", "carbon", "waste", "climate"],
    "Social": ["strike", "harassment", "child labor", "discrimination", "protest", "safety"],
    "Governance": ["lawsuit", "fraud", "corruption", "fine", "bribery", "non-compliance"]
}

# ✅ Example extracted text (replace with real PDF extraction later)
report_texts = {
    "Infosys": """
    Infosys reported initiatives to reduce carbon emissions,
    but still faced criticism over data privacy and employee discrimination.
    No major lawsuits were disclosed in 2023.
    """,

    "Adani": """
    Adani faced lawsuits related to regulatory non-compliance and corruption.
    Environmental groups criticized deforestation and coal emissions.
    Workers reported safety issues at sites.
    """,

    "Honeywell": """
    Honeywell reduced climate impact but still reported waste management challenges.
    No major strikes were reported, but there were cases of harassment complaints.
    Governance risks included bribery allegations.
    """,

    "Wipro": """
    Wipro faced increasing carbon emissions and pollution penalties.
    Social issues included worker protests regarding workplace safety.
    The company was fined by regulators for compliance failures.
    """
}

# ✅ Function to calculate ESG risk counts & percentages
def calculate_esg_percentages(text, esg_dict):
    text = text.lower()
    total_risks = 0
    risk_counts = {category: 0 for category in esg_dict}

    # Count occurrences of each term
    for category, terms in esg_dict.items():
        for term in terms:
            count = len(re.findall(rf"\b{term}\b", text))
            risk_counts[category] += count
            total_risks += count

    # Convert counts to percentages
    if total_risks == 0:
        return risk_counts, {category: 0 for category in esg_dict}

    risk_percentages = {category: (count / total_risks) * 100 for category, count in risk_counts.items()}
    return risk_counts, risk_percentages

# ✅ Run analysis for all companies
results_counts = {}
results_percentages = {}

for company, text in report_texts.items():
    counts, percentages = calculate_esg_percentages(text, esg_terms)
    results_counts[company] = counts
    results_percentages[company] = percentages

# ✅ Convert to DataFrames for better visualization
df_counts = pd.DataFrame(results_counts).T
df_percentages = pd.DataFrame(results_percentages).T

print("=== ESG Risk Counts ===")
print(df_counts)
print("\n=== ESG Risk Percentages ===")
print(df_percentages)